{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911c6180-add1-49f3-a2e2-e11fe5b1c281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/pytorch/2.2.0/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.527\n",
      "[1,   200] loss: 2.301\n",
      "[1,   300] loss: 2.295\n",
      "[1,   400] loss: 2.216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Save the best performing model\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(best_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m    182\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# Calculate the loss from predictions and target labels\u001b[39;00m\n\u001b[1;32m    183\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[1;32m    187\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/optim/adam.py:94\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     has_complex \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_complex(p)\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mparams_with_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Training data filename and labels filename\n",
    "data_file = 'data_train.npy'\n",
    "labels_file = 'labels_train.npy'\n",
    "\n",
    "\n",
    "class MathSymbols(Dataset):\n",
    "    def __init__(self, data_file, labels_file, data_transform=None, label_transform=None):\n",
    "        '''\n",
    "        data_file: Data file for training the model (should be .npy file)\n",
    "        labels_file: Target labels (should be .npy file)   \n",
    "        '''\n",
    "        # Load data \n",
    "        self.data = np.load(data_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        self.data_transform = data_transform\n",
    "        self.label_transform = label_transform\n",
    "        \n",
    "        # Reshape data\n",
    "        self.data = np.reshape(np.transpose(self.data), (4480, 100, 100))\n",
    "\n",
    "        # Convert the images to have 3 channels so it will function with pretrained models\n",
    "        self.data = self.convert_to_rgb(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Length of data (and labels) array\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the dataset item\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = Image.fromarray(sample)  # Convert numpy array to PIL Image\n",
    "        \n",
    "        if self.data_transform:\n",
    "            # Transform the data\n",
    "            sample = self.data_transform(sample)\n",
    "        \n",
    "        if self.label_transform:\n",
    "            # Transform the label\n",
    "            label = self.label_transform(label)\n",
    "        \n",
    "        # Convert sample and label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return sample, label\n",
    "\n",
    "    def convert_to_rgb(self, grayscale_images):\n",
    "        # Add first input channel\n",
    "        grayscale_images = np.expand_dims(grayscale_images, -1)\n",
    "        # Change to 3 input channels to match with pretrained models\n",
    "        rgb_images = grayscale_images.repeat(3, axis=-1)\n",
    "        return rgb_images\n",
    "    \n",
    "    \n",
    "    \n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    \n",
    "def Train(X, Y):\n",
    "    \n",
    "    aug_transform1 = transforms.Compose(\n",
    "    [AutoAugment(policy=AutoAugmentPolicy.IMAGENET),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "    aug_transform2 = transforms.Compose(\n",
    "    [AutoAugment(policy=AutoAugmentPolicy.CIFAR10),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "    aug_transform3 = transforms.Compose(\n",
    "    [AutoAugment(policy=AutoAugmentPolicy.SVHN),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "    \n",
    "    \n",
    "    batch_size = 8\n",
    "\n",
    "    data = MathSymbols(data_file = X, labels_file = Y)\n",
    "\n",
    "    # split for validation\n",
    "    train_len = int((0.8*len(data)))\n",
    "    valid_len = int((len(data)-train_len))\n",
    "\n",
    "    train_data, valid_data = random_split(data, [train_len, valid_len], generator=torch.Generator().manual_seed(1997))\n",
    "\n",
    "    # Apply transformations using the wrapper class\n",
    "    train1 = TransformSubset(train_data, transform=transform)\n",
    "    train2 = TransformSubset(train_data, transform=aug_transform1)\n",
    "    train3 = TransformSubset(train_data, transform=aug_transform2)\n",
    "    train4 = TransformSubset(train_data, transform=aug_transform3)\n",
    "    valid = TransformSubset(valid_data, transform=transform)\n",
    "\n",
    "    train = ConcatDataset([train1, train2, train3, train4])\n",
    "    \n",
    "    #Training data set\n",
    "    trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "    #Validation data set\n",
    "    validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available. Using GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "        \n",
    "    # Load the pre-trained ResNet50 model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    num_classes = 10 \n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    N_epochs = 100\n",
    "    min_loss = np.inf\n",
    "    best_model = None\n",
    "    patience = 30 # How many increases in validation score before stopping training\n",
    "    patience_count = 0\n",
    "\n",
    "    for epoch in range(N_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "        #Training\n",
    "        model.train(True) # Make sure model is in training mode (not eval mode)\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for i, data in enumerate(trainloader):\n",
    "        \n",
    "            # Get the images and target labels\n",
    "            images, labels = data\n",
    "        \n",
    "            # Move images and labels to GPU if possible\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(images) # Calculate predictions for images for this model instantiation\n",
    "            loss = criterion(outputs, labels) # Calculate the loss from predictions and target labels\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval() \n",
    "        val_loss = 0 # Loss\n",
    "        val_num = 0 # Number of classifications made\n",
    "\n",
    "        with torch.no_grad(): # Don't waste memory calculating gradients\n",
    "\n",
    "            for i, data in enumerate(validloader):\n",
    "\n",
    "                images, labels = data\n",
    "                images = images.to(device) #Move images to GPU\n",
    "                labels = labels.to(device) #Move labels to GPU\n",
    "                outputs = model(images) # Classify images with the trained model\n",
    "                loss = criterion(outputs, labels) # Calculate the loss \n",
    "                val_loss += loss.item()\n",
    "                val_num += 1 # Number of validation losses calculated\n",
    "\n",
    "        # Print the average validation loss\n",
    "        avg_val_loss = val_loss / val_num\n",
    "    \n",
    "        # If keep track of lowest avg_loss for \n",
    "        if (avg_val_loss) < min_loss:\n",
    "            min_loss = avg_val_loss \n",
    "            best_model = model.state_dict()\n",
    "            patience_count = 0 # Reset count\n",
    "\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if (patience_count >= patience):\n",
    "                break\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch + 1} validation loss: {avg_val_loss:.3f}')\n",
    "\n",
    "            \n",
    "    print('Finished Training')\n",
    "\n",
    "    # Save the best performing model\n",
    "    torch.save(best_model, 'CNN.pth')\n",
    "    \n",
    "\n",
    "Train(data_file, labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d913a-9fbf-4f6c-bfa7-eeb88899eeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
